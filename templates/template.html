<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Conceptualizing and Measuring Algorithmic Fairness</title>
  </head>

  <body>
    <article id="cover">
      <div style="margin-left: 100px; margin-top: 30px; width: 750px">
        <h1>Conceptualizing and Measuring Algorithmic Fairness</h1>
        <br />
        <div style="font-size: 24px; color: white">
          <h2>Presented by:</h2>
          <h3 style="color: white">Mark A. Zaydman, MD, PhD</h3>
          <p>Assistant Professor of Pathology and Immunology</p>
        </div>
      </div>
    </article>

    <article id="Introduction">
      <section id="general-fairness-definition">
        <h2>General definition of algorithmic fairness</h2>
        <main>
          <div class="column" id="left-column">
            <br />
            <p>
              A fair algorithm will treat people equally without bias or
              favoritism
            </p>
          </div>
          <div class="column" id="right-column">
            <img src="{{assets_dir}}/fairness.jpeg" alt="fairness" />
          </div>
        </main>
      </section>
      <section id="obermeyer-story">
        <main>
          <div class="column">
            <h2>An example of an unfair algorithm</h2>

            <br />
            <p>
              Black patient's had to be sicker than White patients to receive
              the same risk score
            </p>
          </div>
          <div class="column">
            <img src="{{assets_dir}}/obermeyer.jpeg" alt="obermeyer-result" />
            <div class="reference">
              Ziad Obermeyer et al. ,Dissecting racial bias in an algorithm used
              to manage the health of populations.Science366,447-453(2019).
            </div>
          </div>
        </main>
      </section>
      <section id="need-to-measure">
        <h2>
          A formal understanding and definition of algorithmic fairness is
          needed
        </h2>
        <main>
          <div class="column">
            <h5>Motivation</h5>
            <ul>
              <li>Conceptualize</li>
              <li>Benchmarking</li>
              <li>Quality assurance</li>
            </ul>
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/need-to-measure.jpg"
              alt="need-to-measure"
            />
            <div class="referenc">Image created by Dall-E</div>
          </div>
        </main>
      </section>
      <section id="outline">
        <h2>Content of this talk</h2>
        <main>
          <div class="column" id="left-column">
            <ol>
              <li>Define fairness concepts</li>
              <li>Apply fairness concepts</li>
              <li>Operationalizing fairness</li>
            </ol>
          </div>
          <div class="column" id="right-column">
            <img src="{{assets_dir}}/road-map.jpg" alt="road-map" />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section>
    </article>

    <article id="Defining Fairness">
      <section id="fairness-through-unawareness">
        <h2>Fairness through unawareness</h2>
        <main>
          <div class="column" id="column">
            <br />
            <p>
              An algorithm that is blinded to sensitive patient attributes will
              not exhibit biases based on those attributes
            </p>
          </div>
          <div class="column" id="column">
            <img src="{{assets_dir}}/head_in_sand.jpg" alt="head-in-sand" />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section>
      <section id="fairness-through-unawareness-limitations">
        <h2>Fairness through unawareness</h2>
        <main>
          <div class="column" id="credit-column">
            <h4 style="text-decoration: underline">Limitation</h4>
            <p>Unawareness may not be achievable</p>
          </div>
          <div class="column" id="content-column">
            <img
              src="{{assets_dir}}/feature-correlations.png"
              alt="feature-correlations"
              style="width: 700px"
            />
            <div class="reference">
              Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards
              Fair Machine Learning Algorithms in Laboratory Medicine, The
              Journal of Applied Laboratory Medicine, Volume 8, Issue 1, January
              2023, Pages 113–128
            </div>
          </div>
        </main>
      </section>
      <section id="individual-fairness">
        <h2>Individual fairness</h2>
        <main>
          <div class="column">
            <p>"Similar" individuals should be treated "similarly"</p>
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/similar-people.jpg"
              alt="individual-fairness"
            />
            <div class="reference">Created by Dall-E</div>
          </div>
        </main>
      </section>
      <section id="individual-fairness-limitation">
        <h2>Individual fairness</h2>
        <main>
          <div class="column">
            <p>
              Proximal individuals in the input space should map with similar
              probability to the outcomes in the output space
            </p>
            <img
              src="{{assets_dir}}/individual-fairness.png"
              alt="individual-fairness"
            />
          </div>
        </main>
      </section>
      <section id="individual-fairness-limitation">
        <h2>Individual fairness</h2>
        <main>
          <div class="column">
            <p>
              Proximal individuals in the input space should map with similar
              probability to the outcomes in the output space
            </p>
            <img
              src="{{assets_dir}}/individual-fairness.png"
              alt="individual-fairness"
            />
            <br />
            <p
              style="
                text-align: center;
                border: solid;
                border-radius: 10px;
                border-color: #a51417;
              "
            >
              Limitation: the choice of features used to gauge individual
              similarity may introduce bias
            </p>
          </div>
        </main>
      </section>
      <section id="group-fairness">
        <h2>Group fairness</h2>
        <main>
          <div class="column">
            <p>Algorithms should treat different groups of people similarly</p>
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/group-fairness-gender.jpg"
              alt="group-fairness"
            />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section>
      <section id="group-fairness-limitations">
        <h2>Group fairness</h2>
        <main>
          <div class="column">
            <h4>Problems with group labels</h4>
            <ul>
              <li>Societal constructs</li>
              <li>Nonrandom missingness and unreliability</li>
              <li>Group heterogeneity</li>
              <li>Intersectionality</li>
            </ul>
          </div>
          <div class="column">
            <img src="{{assets_dir}}/group-labels-2.jpg" alt="which-group" />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section>
      <section id="demographic-parity">
        <main>
          <div class="column">
            <h2>Demographic parity</h2>
            <br />
            <p>
              The algorithmic output should be independent of group membership
            </p>
            <br /><br />
            <img
              src="{{assets_dir}}/demographic_parity_scorecard.png"
              alt="demographic-parity-scorecard"
              style="width: 350px"
            />
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/demographic_parity_scenario.png"
              alt="demographic-parity-scenario"
              style="width: 450px"
            />
          </div>
        </main>
        <br />
        <div class="reference">
          Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards Fair
          Machine Learning Algorithms in Laboratory Medicine, The Journal of
          Applied Laboratory Medicine, Volume 8, Issue 1, January 2023, Pages
          113–128
        </div>
      </section>
      <section id="equalized-odds">
        <main>
          <div class="column">
            <h2>Equalized odds</h2>
            <br />
            <p>
              The algorithmic output should be independent of group membership
              when conditioned upon the true outcome
            </p>
            <br /><br />
            <img
              src="{{assets_dir}}/equalized_odds_scorecard.png"
              alt="equalized-odds-scorecard"
              style="width: 150px"
            />
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/equalized_odds_scenario.png"
              alt="equalized-odds-scenario"
              style="width: 450px"
            />
          </div>
        </main>
        <br />
        <div class="reference">
          Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards Fair
          Machine Learning Algorithms in Laboratory Medicine, The Journal of
          Applied Laboratory Medicine, Volume 8, Issue 1, January 2023, Pages
          113–128
        </div>
      </section>
      <section id="predictive-parity">
        <main>
          <div class="column">
            <h2>Predictive parity</h2>
            <br />
            <p>
              The probability of the true outcome should be independent of group
              membership when conditioned upon the algorithmic output
            </p>
            <br /><br />
            <img
              src="{{assets_dir}}/pred_parity_scorecard.png"
              alt="predictive-parity-scorecard"
              style="width: 200px"
            />
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/pred_parity_scenario.png"
              alt="predictive-parity-scenario"
              style="width: 425px"
            />
          </div>
        </main>
        <br />
        <div class="reference">
          Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards Fair
          Machine Learning Algorithms in Laboratory Medicine, The Journal of
          Applied Laboratory Medicine, Volume 8, Issue 1, January 2023, Pages
          113–128
        </div>
      </section>
      <section id="group-concept-exclusivity">
        <h2>Exclusivity of group fairness concepts</h2>
        <main>
          <div class="colum">
            <img
              src="{{assets_dir}}/group_fairness.png"
              alt="group-fairness-exclusivity"
            />
          </div>
        </main>
        <div class="reference">
          Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards Fair
          Machine Learning Algorithms in Laboratory Medicine, The Journal of
          Applied Laboratory Medicine, Volume 8, Issue 1, January 2023, Pages
          113–128
        </div>
      </section>
      <section id="equity"></section>
      <section id="concept-summary"></section>
    </article>

    <article id="Obermeyer Study - Fairness ">
      <section id="obermeyer-background"></section>
      <section id="obermeyer-individual-fairness"></section>
      <section id="obermeyer-demographic-parity"></section>
      <section id="obermeyer-equalized-odds"></section>
      <section id="obermeyer-predictive-parity"></section>
      <section id="obermeyer-equity"></section>
    </article>

    <article id="moving forward">
      <section id="building-fairness-infrastructure"></section>
      <section id="call-to-action"></section>
    </article>

    <article id="aknowledgements"></article>
  </body>
</html>
